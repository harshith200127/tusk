{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topsis_bwm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeZWJtUZoe+0ZLmROWwfnU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshith200127/tusk/blob/master/topsis_bwm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-_UVzqbMLNAw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "import pandas as pd\n",
        "\n",
        "class Topsis():\n",
        "    evaluation_matrix = np.array([])  # Matrix\n",
        "    weighted_normalized = np.array([])  # Weight matrix\n",
        "    normalized_decision = np.array([])  # Normalisation matrix\n",
        "    M = 0  # Number of rows\n",
        "    N = 0  # Number of columns\n",
        "\n",
        "    '''\n",
        "\tCreate an evaluation matrix consisting of m alternatives and n criteria,\n",
        "\twith the intersection of each alternative and criteria given as {\\displaystyle x_{ij}}x_{ij},\n",
        "\twe therefore have a matrix {\\displaystyle (x_{ij}){m\\times n}}(x{{ij}})_{{m\\times n}}.\n",
        "\t'''\n",
        "\n",
        "    def __init__(self, evaluation_matrix, weight_matrix, criteria):\n",
        "        # MÃ—N matrix\n",
        "        self.evaluation_matrix = np.array(evaluation_matrix, dtype=\"float\")\n",
        "\n",
        "        # M alternatives (options)\n",
        "        self.row_size = len(self.evaluation_matrix)\n",
        "\n",
        "        # N attributes/criteria\n",
        "        self.column_size = len(self.evaluation_matrix[0])\n",
        "\n",
        "        # N size weight matrix\n",
        "        self.weight_matrix = np.array(weight_matrix, dtype=\"float\")\n",
        "        self.weight_matrix = self.weight_matrix/sum(self.weight_matrix)\n",
        "        self.criteria = np.array(criteria, dtype=\"float\")\n",
        "\n",
        "    '''\n",
        "\t# Step 2\n",
        "\tThe matrix {\\displaystyle (x_{ij}){m\\times n}}(x{{ij}})_{{m\\times n}} is then normalised to form the matrix\n",
        "\t'''\n",
        "\n",
        "    def step_2(self):\n",
        "        # normalized scores\n",
        "        self.normalized_decision = np.copy(self.evaluation_matrix)\n",
        "        sqrd_sum = np.zeros(self.column_size)\n",
        "        for i in range(self.row_size):\n",
        "            for j in range(self.column_size):\n",
        "                sqrd_sum[j] += self.evaluation_matrix[i, j]**2\n",
        "        for i in range(self.row_size):\n",
        "            for j in range(self.column_size):\n",
        "                self.normalized_decision[i,\n",
        "                                         j] = self.evaluation_matrix[i, j]/(sqrd_sum[j]**0.5)\n",
        "\n",
        "    '''\n",
        "\t# Step 3\n",
        "\tCalculate the weighted normalised decision matrix\n",
        "\t'''\n",
        "\n",
        "    def step_3(self):\n",
        "        from pdb import set_trace\n",
        "        self.weighted_normalized = np.copy(self.normalized_decision)\n",
        "        for i in range(self.row_size):\n",
        "            for j in range(self.column_size):\n",
        "                self.weighted_normalized[i, j] *= self.weight_matrix[j]\n",
        "\n",
        "    '''\n",
        "\t# Step 4\n",
        "\tDetermine the worst alternative {\\displaystyle (A_{w})}(A_{w}) and the best alternative {\\displaystyle (A_{b})}(A_{b}):\n",
        "\t'''\n",
        "\n",
        "    def step_4(self):\n",
        "        self.worst_alternatives = np.zeros(self.column_size)\n",
        "        self.best_alternatives = np.zeros(self.column_size)\n",
        "        for i in range(self.column_size):\n",
        "            if self.criteria[i]:\n",
        "                self.worst_alternatives[i] = min(\n",
        "                    self.weighted_normalized[:, i])\n",
        "                self.best_alternatives[i] = max(self.weighted_normalized[:, i])\n",
        "            else:\n",
        "                self.worst_alternatives[i] = max(\n",
        "                    self.weighted_normalized[:, i])\n",
        "                self.best_alternatives[i] = min(self.weighted_normalized[:, i])\n",
        "\n",
        "    '''\n",
        "\t# Step 5\n",
        "\tCalculate the L2-distance between the target alternative {\\displaystyle i}i and the worst condition {\\displaystyle A_{w}}A_{w}\n",
        "\t{\\displaystyle d_{iw}={\\sqrt {\\sum {j=1}^{n}(t{ij}-t_{wj})^{2}}},\\quad i=1,2,\\ldots ,m,}\n",
        "\tand the distance between the alternative {\\displaystyle i}i and the best condition {\\displaystyle A_{b}}A_b\n",
        "\t{\\displaystyle d_{ib}={\\sqrt {\\sum {j=1}^{n}(t{ij}-t_{bj})^{2}}},\\quad i=1,2,\\ldots ,m}\n",
        "\twhere {\\displaystyle d_{iw}}d_{{iw}} and {\\displaystyle d_{ib}}d_{{ib}} are L2-norm distances \n",
        "\tfrom the target alternative {\\displaystyle i}i to the worst and best conditions, respectively.\n",
        "\t'''\n",
        "\n",
        "    def step_5(self):\n",
        "        self.worst_distance = np.zeros(self.row_size)\n",
        "        self.best_distance = np.zeros(self.row_size)\n",
        "\n",
        "        self.worst_distance_mat = np.copy(self.weighted_normalized)\n",
        "        self.best_distance_mat = np.copy(self.weighted_normalized)\n",
        "\n",
        "        for i in range(self.row_size):\n",
        "            for j in range(self.column_size):\n",
        "                self.worst_distance_mat[i][j] = (self.weighted_normalized[i][j]-self.worst_alternatives[j])**2\n",
        "                self.best_distance_mat[i][j] = (self.weighted_normalized[i][j]-self.best_alternatives[j])**2\n",
        "                \n",
        "                self.worst_distance[i] += self.worst_distance_mat[i][j]\n",
        "                self.best_distance[i] += self.best_distance_mat[i][j]\n",
        "\n",
        "        for i in range(self.row_size):\n",
        "            self.worst_distance[i] = self.worst_distance[i]**0.5\n",
        "            self.best_distance[i] = self.best_distance[i]**0.5\n",
        "\n",
        "    '''\n",
        "\t# Step 6\n",
        "\tCalculate the similarity\n",
        "\t'''\n",
        "\n",
        "    def step_6(self):\n",
        "        np.seterr(all='ignore')\n",
        "        self.worst_similarity = np.zeros(self.row_size)\n",
        "        self.best_similarity = np.zeros(self.row_size)\n",
        "\n",
        "        for i in range(self.row_size):\n",
        "            # calculate the similarity to the worst condition\n",
        "            self.worst_similarity[i] = self.worst_distance[i] / \\\n",
        "                (self.worst_distance[i]+self.best_distance[i])\n",
        "\n",
        "            # calculate the similarity to the best condition\n",
        "            self.best_similarity[i] = self.best_distance[i] / \\\n",
        "                (self.worst_distance[i]+self.best_distance[i])\n",
        "    \n",
        "    def ranking(self, data):\n",
        "        return [i+1 for i in data.argsort()]\n",
        "\n",
        "    def rank_to_worst_similarity(self):\n",
        "        # return rankdata(self.worst_similarity, method=\"min\").astype(int)\n",
        "        return self.ranking(self.worst_similarity)\n",
        "\n",
        "    def rank_to_best_similarity(self):\n",
        "        # return rankdata(self.best_similarity, method='min').astype(int)\n",
        "        return self.ranking(self.best_similarity)\n",
        "\n",
        "    def calc(self):\n",
        "        print(\"Step 1\\n\", self.evaluation_matrix, end=\"\\n\\n\")\n",
        "        self.step_2()\n",
        "        print(\"Step 2\\n\", self.normalized_decision, end=\"\\n\\n\")\n",
        "        self.step_3()\n",
        "        print(\"Step 3\\n\", self.weighted_normalized, end=\"\\n\\n\")\n",
        "        self.step_4()\n",
        "        print(\"Step 4\\n\", self.worst_alternatives,\n",
        "              self.best_alternatives, end=\"\\n\\n\")\n",
        "        self.step_5()\n",
        "        print(\"Step 5\\n\", self.worst_distance, self.best_distance, end=\"\\n\\n\")\n",
        "        self.step_6()\n",
        "        print(\"Step 6\\n\", self.worst_similarity,\n",
        "              self.best_similarity, end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import math\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "# Function\n",
        "def target_function():\n",
        "    return\n",
        "\n",
        "############################################################################\n",
        "\n",
        "# Function: Initialize Variables\n",
        "def initial_position(pack_size = 5, min_values = [-5,-5], max_values = [5,5], target_function = target_function):\n",
        "    position = np.zeros((pack_size, len(min_values)+1))\n",
        "    for i in range(0, pack_size):\n",
        "        for j in range(0, len(min_values)):\n",
        "             position[i,j] = random.uniform(min_values[j], max_values[j])\n",
        "        position[i,-1] = target_function(position[i,0:position.shape[1]-1])\n",
        "    return position\n",
        "\n",
        "# Function: Initialize Alpha\n",
        "def alpha_position(dimension = 2, target_function = target_function):\n",
        "    alpha = np.zeros((1, dimension + 1))\n",
        "    for j in range(0, dimension):\n",
        "        alpha[0,j] = 0.0\n",
        "    alpha[0,-1] = target_function(alpha[0,0:alpha.shape[1]-1])\n",
        "    return alpha\n",
        "\n",
        "# Function: Initialize Beta\n",
        "def beta_position(dimension = 2, target_function = target_function):\n",
        "    beta = np.zeros((1, dimension + 1))\n",
        "    for j in range(0, dimension):\n",
        "        beta[0,j] = 0.0\n",
        "    beta[0,-1] = target_function(beta[0,0:beta.shape[1]-1])\n",
        "    return beta\n",
        "\n",
        "# Function: Initialize Delta\n",
        "def delta_position(dimension = 2, target_function = target_function):\n",
        "    delta =  np.zeros((1, dimension + 1))\n",
        "    for j in range(0, dimension):\n",
        "        delta[0,j] = 0.0\n",
        "    delta[0,-1] = target_function(delta[0,0:delta.shape[1]-1])\n",
        "    return delta\n",
        "\n",
        "# Function: Updtade Pack by Fitness\n",
        "def update_pack(position, alpha, beta, delta):\n",
        "    updated_position = np.copy(position)\n",
        "    for i in range(0, position.shape[0]):\n",
        "        if (updated_position[i,-1] < alpha[0,-1]):\n",
        "            alpha[0,:] = np.copy(updated_position[i,:])\n",
        "        if (updated_position[i,-1] > alpha[0,-1] and updated_position[i,-1] < beta[0,-1]):\n",
        "            beta[0,:] = np.copy(updated_position[i,:])\n",
        "        if (updated_position[i,-1] > alpha[0,-1] and updated_position[i,-1] > beta[0,-1]  and updated_position[i,-1] < delta[0,-1]):\n",
        "            delta[0,:] = np.copy(updated_position[i,:])\n",
        "    return alpha, beta, delta\n",
        "\n",
        "# Function: Updtade Position\n",
        "def update_position(position, alpha, beta, delta, a_linear_component = 2, min_values = [-5,-5], max_values = [5,5], target_function = target_function):\n",
        "    updated_position = np.copy(position)\n",
        "    for i in range(0, updated_position.shape[0]):\n",
        "        for j in range (0, len(min_values)):   \n",
        "            r1_alpha              = int.from_bytes(os.urandom(8), byteorder = \"big\") / ((1 << 64) - 1)\n",
        "            r2_alpha              = int.from_bytes(os.urandom(8), byteorder = \"big\") / ((1 << 64) - 1)\n",
        "            a_alpha               = 2*a_linear_component*r1_alpha - a_linear_component\n",
        "            c_alpha               = 2*r2_alpha      \n",
        "            distance_alpha        = abs(c_alpha*alpha[0,j] - position[i,j]) \n",
        "            x1                    = alpha[0,j] - a_alpha*distance_alpha   \n",
        "            r1_beta               = int.from_bytes(os.urandom(8), byteorder = \"big\") / ((1 << 64) - 1)\n",
        "            r2_beta               = int.from_bytes(os.urandom(8), byteorder = \"big\") / ((1 << 64) - 1)\n",
        "            a_beta                = 2*a_linear_component*r1_beta - a_linear_component\n",
        "            c_beta                = 2*r2_beta            \n",
        "            distance_beta         = abs(c_beta*beta[0,j] - position[i,j]) \n",
        "            x2                    = beta[0,j] - a_beta*distance_beta                          \n",
        "            r1_delta              = int.from_bytes(os.urandom(8), byteorder = \"big\") / ((1 << 64) - 1)\n",
        "            r2_delta              = int.from_bytes(os.urandom(8), byteorder = \"big\") / ((1 << 64) - 1)\n",
        "            a_delta               = 2*a_linear_component*r1_delta - a_linear_component\n",
        "            c_delta               = 2*r2_delta            \n",
        "            distance_delta        = abs(c_delta*delta[0,j] - position[i,j]) \n",
        "            x3                    = delta[0,j] - a_delta*distance_delta                                 \n",
        "            updated_position[i,j] = np.clip(((x1 + x2 + x3)/3),min_values[j],max_values[j])     \n",
        "        updated_position[i,-1] = target_function(updated_position[i,0:updated_position.shape[1]-1])\n",
        "    return updated_position\n",
        "\n",
        "############################################################################\n",
        "\n",
        "# GWO Function\n",
        "def grey_wolf_optimizer(pack_size = 5, min_values = [-5,-5], max_values = [5,5], iterations = 50, target_function = target_function, verbose = True):    \n",
        "    count    = 0\n",
        "    alpha    = alpha_position(dimension = len(min_values), target_function = target_function)\n",
        "    beta     = beta_position(dimension  = len(min_values), target_function = target_function)\n",
        "    delta    = delta_position(dimension = len(min_values), target_function = target_function)\n",
        "    position = initial_position(pack_size = pack_size, min_values = min_values, max_values = max_values, target_function = target_function)\n",
        "    while (count <= iterations): \n",
        "        if (verbose == True):    \n",
        "            print('Iteration = ', count, ' f(x) = ', alpha[0][-1])      \n",
        "        a_linear_component = 2 - count*(2/iterations)\n",
        "        alpha, beta, delta = update_pack(position, alpha, beta, delta)\n",
        "        position           = update_position(position, alpha, beta, delta, a_linear_component = a_linear_component, min_values = min_values, max_values = max_values, target_function = target_function)    \n",
        "        count              = count + 1          \n",
        "    return alpha\n",
        "import numpy as np\n",
        "\n",
        "#from pyDecision.util.gwo import grey_wolf_optimizer\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# Function: BWM\n",
        "def bw_method(dataset, mic, lic, size = 50, iterations = 150):\n",
        "    X         = np.copy(dataset)/1.0\n",
        "    best      = np.where(mic == 1)[0][0]\n",
        "    worst     = np.where(lic == 1)[0][0]\n",
        "    pairs_b   = [(best, i)  for i in range(0, mic.shape[0])]\n",
        "    pairs_w   = [(i, worst) for i in range(0, mic.shape[0]) if (i, worst) not in pairs_b]\n",
        "    def target_function(variables):\n",
        "        eps       = [float('+inf')]\n",
        "        for pair in pairs_b:\n",
        "            i, j = pair\n",
        "            diff = abs(variables[i] - variables[j]*mic[j])\n",
        "            if ( i != j):\n",
        "                eps.append(diff)\n",
        "        for pair in pairs_w:\n",
        "            i, j = pair\n",
        "            diff = abs(variables[i] - variables[j]*lic[j])\n",
        "            if ( i != j):\n",
        "                eps.append(diff)\n",
        "        if ( np.sum(variables) == 0):\n",
        "            eps = float('+inf')\n",
        "        else:\n",
        "            eps = max(eps[1:])\n",
        "        return eps\n",
        "    weights = grey_wolf_optimizer(pack_size = size, min_values = [0.01]*X.shape[1], max_values = [1]*X.shape[1], iterations = iterations, target_function = target_function)\n",
        "    weights = weights[0][:-1]/sum(weights[0][:-1])\n",
        "    return weights\n",
        "\n"
      ],
      "metadata": {
        "id": "n6aA35YWLZ7y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import files\n",
        " \n",
        " \n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "9IWs759EPCvZ",
        "outputId": "ba71bd44-0c36-46df-96fa-1438fa0e1b17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a65fecf3-dabd-4e90-8782-a105f968b510\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a65fecf3-dabd-4e90-8782-a105f968b510\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving demo.csv to demo.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_matrix=pd.read_csv(\"demo.csv\")\n",
        "mic = np.array([1, 3, 4, 7])\n",
        "\n",
        "lic = np.array([7, 5, 5, 1])\n",
        "\n",
        "#weights = [0.25, 0.25, 0.25, 0.25]\n",
        "weights =  bw_method(evaluation_matrix, mic, lic, size = 50, iterations = 150)\n",
        "print(\"Weights :\" ,weights)\n",
        "'''\n",
        "if higher value is preferred - True\n",
        "if lower value is preferred - False\n",
        "'''\n",
        "criterias = np.array([True, True, True, True])\n",
        "\n",
        "t = Topsis(evaluation_matrix, weights, criterias)\n",
        "\n",
        "t.calc()\n",
        "\n",
        "print(\"best_distance\\t\", t.best_distance)\n",
        "print(\"worst_distance\\t\", t.worst_distance)\n",
        "\n",
        "# print(\"weighted_normalized\",t.weighted_normalized)\n",
        "\n",
        "print(\"worst_similarity\\t\", t.worst_similarity)\n",
        "print(\"rank_to_worst_similarity\\t\", t.rank_to_worst_similarity())\n",
        "\n",
        "print(\"best_similarity\\t\", t.best_similarity)\n",
        "print(\"rank_to_best_similarity\\t\", t.rank_to_best_similarity())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7bxUgQrPDTp",
        "outputId": "187c9e9f-2a91-487e-a0da-988f4c191ead"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration =  0  f(x) =  inf\n",
            "Iteration =  1  f(x) =  0.7171570955363722\n",
            "Iteration =  2  f(x) =  0.4463617845382532\n",
            "Iteration =  3  f(x) =  0.15991945921485734\n",
            "Iteration =  4  f(x) =  0.15991945921485734\n",
            "Iteration =  5  f(x) =  0.055970708626690964\n",
            "Iteration =  6  f(x) =  0.055970708626690964\n",
            "Iteration =  7  f(x) =  0.04014773466929567\n",
            "Iteration =  8  f(x) =  0.04014773466929567\n",
            "Iteration =  9  f(x) =  0.02815167272843204\n",
            "Iteration =  10  f(x) =  0.02815167272843204\n",
            "Iteration =  11  f(x) =  0.02815167272843204\n",
            "Iteration =  12  f(x) =  0.02815167272843204\n",
            "Iteration =  13  f(x) =  0.02815167272843204\n",
            "Iteration =  14  f(x) =  0.02815167272843204\n",
            "Iteration =  15  f(x) =  0.02815167272843204\n",
            "Iteration =  16  f(x) =  0.02815167272843204\n",
            "Iteration =  17  f(x) =  0.02815167272843204\n",
            "Iteration =  18  f(x) =  0.02815167272843204\n",
            "Iteration =  19  f(x) =  0.02815167272843204\n",
            "Iteration =  20  f(x) =  0.02815167272843204\n",
            "Iteration =  21  f(x) =  0.02815167272843204\n",
            "Iteration =  22  f(x) =  0.02815167272843204\n",
            "Iteration =  23  f(x) =  0.02815167272843204\n",
            "Iteration =  24  f(x) =  0.026371832309439895\n",
            "Iteration =  25  f(x) =  0.026371832309439895\n",
            "Iteration =  26  f(x) =  0.023550785093955903\n",
            "Iteration =  27  f(x) =  0.023550785093955903\n",
            "Iteration =  28  f(x) =  0.022973618380383316\n",
            "Iteration =  29  f(x) =  0.01813818685755468\n",
            "Iteration =  30  f(x) =  0.01813818685755468\n",
            "Iteration =  31  f(x) =  0.016401688662545384\n",
            "Iteration =  32  f(x) =  0.016401688662545384\n",
            "Iteration =  33  f(x) =  0.016401688662545384\n",
            "Iteration =  34  f(x) =  0.01599793280821233\n",
            "Iteration =  35  f(x) =  0.015138316432623205\n",
            "Iteration =  36  f(x) =  0.014550873219189636\n",
            "Iteration =  37  f(x) =  0.014550873219189636\n",
            "Iteration =  38  f(x) =  0.014550873219189636\n",
            "Iteration =  39  f(x) =  0.012360101262076224\n",
            "Iteration =  40  f(x) =  0.012360101262076224\n",
            "Iteration =  41  f(x) =  0.012360101262076224\n",
            "Iteration =  42  f(x) =  0.012360101262076224\n",
            "Iteration =  43  f(x) =  0.012360101262076224\n",
            "Iteration =  44  f(x) =  0.01006676602003407\n",
            "Iteration =  45  f(x) =  0.01006676602003407\n",
            "Iteration =  46  f(x) =  0.01006676602003407\n",
            "Iteration =  47  f(x) =  0.01006676602003407\n",
            "Iteration =  48  f(x) =  0.009990587654318995\n",
            "Iteration =  49  f(x) =  0.009990587654318995\n",
            "Iteration =  50  f(x) =  0.009990587654318995\n",
            "Iteration =  51  f(x) =  0.009990587654318995\n",
            "Iteration =  52  f(x) =  0.009990587654318995\n",
            "Iteration =  53  f(x) =  0.009990587654318995\n",
            "Iteration =  54  f(x) =  0.009990587654318995\n",
            "Iteration =  55  f(x) =  0.00974607151711803\n",
            "Iteration =  56  f(x) =  0.00974607151711803\n",
            "Iteration =  57  f(x) =  0.009686073275272556\n",
            "Iteration =  58  f(x) =  0.009686073275272556\n",
            "Iteration =  59  f(x) =  0.009686073275272556\n",
            "Iteration =  60  f(x) =  0.009686073275272556\n",
            "Iteration =  61  f(x) =  0.009686073275272556\n",
            "Iteration =  62  f(x) =  0.009686073275272556\n",
            "Iteration =  63  f(x) =  0.009686073275272556\n",
            "Iteration =  64  f(x) =  0.008738410935775444\n",
            "Iteration =  65  f(x) =  0.008738410935775444\n",
            "Iteration =  66  f(x) =  0.008738410935775444\n",
            "Iteration =  67  f(x) =  0.008738410935775444\n",
            "Iteration =  68  f(x) =  0.008738410935775444\n",
            "Iteration =  69  f(x) =  0.008738410935775444\n",
            "Iteration =  70  f(x) =  0.008738410935775444\n",
            "Iteration =  71  f(x) =  0.008738410935775444\n",
            "Iteration =  72  f(x) =  0.008738410935775444\n",
            "Iteration =  73  f(x) =  0.008738410935775444\n",
            "Iteration =  74  f(x) =  0.008738410935775444\n",
            "Iteration =  75  f(x) =  0.008738410935775444\n",
            "Iteration =  76  f(x) =  0.008738410935775444\n",
            "Iteration =  77  f(x) =  0.008738410935775444\n",
            "Iteration =  78  f(x) =  0.008738410935775444\n",
            "Iteration =  79  f(x) =  0.008738410935775444\n",
            "Iteration =  80  f(x) =  0.008738410935775444\n",
            "Iteration =  81  f(x) =  0.008738410935775444\n",
            "Iteration =  82  f(x) =  0.008738410935775444\n",
            "Iteration =  83  f(x) =  0.008738410935775444\n",
            "Iteration =  84  f(x) =  0.008738410935775444\n",
            "Iteration =  85  f(x) =  0.008738410935775444\n",
            "Iteration =  86  f(x) =  0.008738410935775444\n",
            "Iteration =  87  f(x) =  0.008738410935775444\n",
            "Iteration =  88  f(x) =  0.008738410935775444\n",
            "Iteration =  89  f(x) =  0.008738410935775444\n",
            "Iteration =  90  f(x) =  0.008621538909349252\n",
            "Iteration =  91  f(x) =  0.008621538909349252\n",
            "Iteration =  92  f(x) =  0.008621538909349252\n",
            "Iteration =  93  f(x) =  0.008621538909349252\n",
            "Iteration =  94  f(x) =  0.008621538909349252\n",
            "Iteration =  95  f(x) =  0.008621538909349252\n",
            "Iteration =  96  f(x) =  0.008621538909349252\n",
            "Iteration =  97  f(x) =  0.008621538909349252\n",
            "Iteration =  98  f(x) =  0.008621538909349252\n",
            "Iteration =  99  f(x) =  0.008487726369161015\n",
            "Iteration =  100  f(x) =  0.008487726369161015\n",
            "Iteration =  101  f(x) =  0.008487726369161015\n",
            "Iteration =  102  f(x) =  0.008487726369161015\n",
            "Iteration =  103  f(x) =  0.008205480451446907\n",
            "Iteration =  104  f(x) =  0.008205480451446907\n",
            "Iteration =  105  f(x) =  0.008205480451446907\n",
            "Iteration =  106  f(x) =  0.008205480451446907\n",
            "Iteration =  107  f(x) =  0.008205480451446907\n",
            "Iteration =  108  f(x) =  0.008205480451446907\n",
            "Iteration =  109  f(x) =  0.008205480451446907\n",
            "Iteration =  110  f(x) =  0.008205480451446907\n",
            "Iteration =  111  f(x) =  0.008205480451446907\n",
            "Iteration =  112  f(x) =  0.008205480451446907\n",
            "Iteration =  113  f(x) =  0.008205480451446907\n",
            "Iteration =  114  f(x) =  0.008205480451446907\n",
            "Iteration =  115  f(x) =  0.008105230655179513\n",
            "Iteration =  116  f(x) =  0.008105230655179513\n",
            "Iteration =  117  f(x) =  0.008105230655179513\n",
            "Iteration =  118  f(x) =  0.008105230655179513\n",
            "Iteration =  119  f(x) =  0.008105230655179513\n",
            "Iteration =  120  f(x) =  0.008105230655179513\n",
            "Iteration =  121  f(x) =  0.008105230655179513\n",
            "Iteration =  122  f(x) =  0.008105230655179513\n",
            "Iteration =  123  f(x) =  0.008105230655179513\n",
            "Iteration =  124  f(x) =  0.008105230655179513\n",
            "Iteration =  125  f(x) =  0.008105230655179513\n",
            "Iteration =  126  f(x) =  0.008105230655179513\n",
            "Iteration =  127  f(x) =  0.008105230655179513\n",
            "Iteration =  128  f(x) =  0.008105230655179513\n",
            "Iteration =  129  f(x) =  0.008105230655179513\n",
            "Iteration =  130  f(x) =  0.008105230655179513\n",
            "Iteration =  131  f(x) =  0.008105230655179513\n",
            "Iteration =  132  f(x) =  0.008105230655179513\n",
            "Iteration =  133  f(x) =  0.008103295833235907\n",
            "Iteration =  134  f(x) =  0.008103295833235907\n",
            "Iteration =  135  f(x) =  0.008103295833235907\n",
            "Iteration =  136  f(x) =  0.008103295833235907\n",
            "Iteration =  137  f(x) =  0.008103295833235907\n",
            "Iteration =  138  f(x) =  0.008103295833235907\n",
            "Iteration =  139  f(x) =  0.008103295833235907\n",
            "Iteration =  140  f(x) =  0.008103295833235907\n",
            "Iteration =  141  f(x) =  0.008103295833235907\n",
            "Iteration =  142  f(x) =  0.008094212634859271\n",
            "Iteration =  143  f(x) =  0.008094212634859271\n",
            "Iteration =  144  f(x) =  0.008094212634859271\n",
            "Iteration =  145  f(x) =  0.008094212634859271\n",
            "Iteration =  146  f(x) =  0.00808565494194812\n",
            "Iteration =  147  f(x) =  0.00808565494194812\n",
            "Iteration =  148  f(x) =  0.008019024146867941\n",
            "Iteration =  149  f(x) =  0.008019024146867941\n",
            "Iteration =  150  f(x) =  0.008019024146867941\n",
            "Weights : [0.58546949 0.1700114  0.15007757 0.09444154]\n",
            "Step 1\n",
            " [[250.  16.  12.   5.]\n",
            " [200.  16.   8.   3.]\n",
            " [300.  32.  16.   4.]\n",
            " [275.  32.   8.   4.]\n",
            " [225.  16.  16.   2.]]\n",
            "\n",
            "Step 2\n",
            " [[0.44280744 0.30151134 0.42857143 0.5976143 ]\n",
            " [0.35424595 0.30151134 0.28571429 0.35856858]\n",
            " [0.53136893 0.60302269 0.57142857 0.47809144]\n",
            " [0.48708819 0.60302269 0.28571429 0.47809144]\n",
            " [0.3985267  0.30151134 0.57142857 0.23904572]]\n",
            "\n",
            "Step 3\n",
            " [[0.25925025 0.05126037 0.06431896 0.05643962]\n",
            " [0.2074002  0.05126037 0.04287931 0.03386377]\n",
            " [0.3111003  0.10252073 0.08575861 0.04515169]\n",
            " [0.28517527 0.10252073 0.04287931 0.04515169]\n",
            " [0.23332522 0.05126037 0.08575861 0.02257585]]\n",
            "\n",
            "Step 4\n",
            " [0.2074002  0.05126037 0.04287931 0.02257585] [0.3111003  0.10252073 0.08575861 0.05643962]\n",
            "\n",
            "Step 5\n",
            " [0.06553504 0.01128792 0.12541786 0.09584496 0.0501073 ] [0.0759981  0.12541786 0.01128792 0.05136301 0.09911278]\n",
            "\n",
            "Step 6\n",
            " [0.4630367  0.08257093 0.91742907 0.65108539 0.33579464] [0.5369633  0.91742907 0.08257093 0.34891461 0.66420536]\n",
            "\n",
            "best_distance\t [0.0759981  0.12541786 0.01128792 0.05136301 0.09911278]\n",
            "worst_distance\t [0.06553504 0.01128792 0.12541786 0.09584496 0.0501073 ]\n",
            "worst_similarity\t [0.4630367  0.08257093 0.91742907 0.65108539 0.33579464]\n",
            "rank_to_worst_similarity\t [2, 5, 1, 4, 3]\n",
            "best_similarity\t [0.5369633  0.91742907 0.08257093 0.34891461 0.66420536]\n",
            "rank_to_best_similarity\t [3, 4, 1, 5, 2]\n"
          ]
        }
      ]
    }
  ]
}